trigger:
  branches:
    include:
      - dev
      - master

pr:
  branches:
    include:
      - dev
      - master

jobs:
  - job: Linux
    pool:
      vmImage: 'ubuntu-16.04'
      
    steps:
      - task: GoTool@0
        inputs:
          version: '1.15'
      - script: |
          GOARCH=arm64 GOOS=linux go build -o azcopy_linux_arm64
        displayName: 'Generate builds'
        
  - job: AArch64_Manylinux2014Build
    pool:
      vmImage: 'ubuntu-18.04'

    steps:
    - script: docker run --rm --privileged hypriot/qemu-register
      displayName: 'Registering qemu'
    - script: |
        docker run -v $(pwd):"${DOCKER_ROOT_DIRECTORY}":rw,z \
                   -e HOST_USER_ID \
                    "quay.io/pypa/manylinux2014_aarch64:latest" \
                    bash -c "cd $DOCKER_ROOT_DIRECTORY;
                    "${PYBIN}/pip" install --upgrade pip && \
                    "${PYBIN}/pip" install twine numpy && \
                    "${PYBIN}/pip" wheel . -w wheelhouse/ && \
                    auditwheel repair wheelhouse/deap*.whl --plat manylinux2014_aarch64 -w wheelhouse-manylinux && \
                    "${PYBIN}/python" -m twine upload -r pypi -u $(twineUsername) -p $(twinePassword) --skip-existing --disable-progress-bar wheelhouse-manylinux/*"
      displayName: 'Running AArch64 build'
      env:
        DOCKER_ROOT_DIRECTORY: "/home/source_root"
        HOST_USER_ID: $(id -u)
        PYBIN: /opt/python/$(python.version)/bin
        
      #- task: PublishBuildArtifacts@1
       # displayName: 'Publish Artifacts'
        #condition: succeededOrFailed()

  #- job: MacOS_Build
   # pool:
    #  vmImage: 'macOS-10.14'
    #steps:
     # - task: GoTool@0
      #  inputs:
       #   version: '1.15'
      #- script: |
       #   go build -o "$(Build.ArtifactStagingDirectory)/azcopy_darwin_amd64"
        #displayName: 'Generate builds'

      #- task: PublishBuildArtifacts@1
       # displayName: 'Publish Artifacts'
       # condition: succeededOrFailed()

  #- job: Test_On_Ubuntu
   # variables:
    #  isMutexSet: 'false'
    # allow maximum build time, in case we have build congestion
    #timeoutInMinutes: 360
    #pool:
     # vmImage: 'ubuntu-16.04'
    #steps:
     # - task: UsePythonVersion@0
      #  name: 'Set_up_Python'
       # inputs:
        #  versionSpec: '3.7'
      #- task: GoTool@0
       # name: 'Set_up_Golang'
        #inputs:
         # version: '1.15'
     # - task: DownloadSecureFile@1
      #  name: ciGCSServiceAccountKey
       # displayName: 'Download GCS Service Account Key'
        #inputs:
         # secureFile: 'ci-gcs-dev.json'
      #- script: |
       #   pip install azure-storage-blob==12.0.0b3
          # the recent release 1.0.0b4 has a breaking change
        #  pip install azure-core==1.0.0b3
          # acquire the mutex before running live tests to avoid conflicts
         # python ./tool_distributed_mutex.py lock "$(MUTEX_URL)"
          # set the variable to indicate that the mutex was actually acquired
          #echo '##vso[task.setvariable variable=isMutexSet]true'
       # name: 'Acquire_the_distributed_mutex'
      #- script: |
          # run unit test and build executable
          # the set -e line is needed so that the unit tests failure would cause the job to fail properly
          # "-check.v" (must be after package list) outputs timings
       #   set -e
        #  go test -timeout 45m -race -short -cover ./cmd ./common ./common/parallel ./ste ./azbfs ./sddl "-check.v"
         # GOARCH=amd64 GOOS=linux go build -o azcopy_linux_amd64
       # name: 'Run_unit_tests'
        #env:
         # ACCOUNT_NAME: $(ACCOUNT_NAME)
          #ACCOUNT_KEY: $(ACCOUNT_KEY)
          #AZCOPY_E2E_ACCOUNT_KEY: $(AZCOPY_E2E_ACCOUNT_KEY)
          #AZCOPY_E2E_ACCOUNT_NAME: $(AZCOPY_E2E_ACCOUNT_NAME)
          #AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
          #AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
          #GOOGLE_APPLICATION_CREDENTIALS: $(ciGCSServiceAccountKey.secureFilePath)
          #GOOGLE_CLOUD_PROJECT: $(GOOGLE_CLOUD_PROJECT)
      #- script: |
       #   set -e
        #  GOARCH=amd64 GOOS=linux go build -o azcopy_linux_amd64
         # export AZCOPY_E2E_EXECUTABLE_PATH=$(pwd)/azcopy_linux_amd64
          #go test -timeout 20m -race -short -cover ./e2etest         
        #name: 'Run_e2e_tests'
        #env:
         # AZCOPY_E2E_ACCOUNT_KEY: $(AZCOPY_E2E_ACCOUNT_KEY)
          #AZCOPY_E2E_ACCOUNT_NAME: $(AZCOPY_E2E_ACCOUNT_NAME)
      #- script: |
       #   go build -o test-validator ./testSuite/
        #  mkdir test-temp
         # export AZCOPY_EXECUTABLE_PATH=$(pwd)/azcopy_linux_amd64
          #export TEST_SUITE_EXECUTABLE_LOCATION=$(pwd)/test-validator
          #export TEST_DIRECTORY_PATH=$(pwd)/test-temp

          # install the CLFSLoad extension
         # pip3 install clfsload

          #keyctl session test python ./testSuite/scripts/run.py
        #name: 'Run_smoke_tests'
        #env:
         # ACCOUNT_NAME: $(ACCOUNT_NAME)
          #ACCOUNT_KEY: $(ACCOUNT_KEY)
          #AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
          #AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
          #GOOGLE_CLOUD_PROJECT: $(GOOGLE_CLOUD_PROJECT)
          #GOOGLE_APPLICATION_CREDENTIALS: $(ciGCSServiceAccountKey.secureFilePath)
          #ACTIVE_DIRECTORY_APPLICATION_ID: $(ACTIVE_DIRECTORY_APPLICATION_ID)
          #AZCOPY_SPA_CLIENT_SECRET: $(AZCOPY_SPA_CLIENT_SECRET)
          #CONTAINER_OAUTH_URL: $(CONTAINER_OAUTH_URL)
          #CONTAINER_OAUTH_VALIDATE_SAS_URL: $(CONTAINER_OAUTH_VALIDATE_SAS_URL)
          #CONTAINER_SAS_URL: $(CONTAINER_SAS_URL)
          #FILESYSTEM_SAS_URL: $(FILESYSTEM_SAS_URL)
          #FILESYSTEM_URL: $(FILESYSTEM_URL)
          #OAUTH_AAD_ENDPOINT: $(OAUTH_AAD_ENDPOINT)
          #OAUTH_TENANT_ID: $(OAUTH_TENANT_ID)
          #PREMIUM_CONTAINER_SAS_URL: $(PREMIUM_CONTAINER_SAS_URL)
          #S2S_DST_BLOB_ACCOUNT_SAS_URL: $(S2S_DST_BLOB_ACCOUNT_SAS_URL)
          #S2S_SRC_BLOB_ACCOUNT_SAS_URL: $(S2S_SRC_BLOB_ACCOUNT_SAS_URL)
          #S2S_SRC_FILE_ACCOUNT_SAS_URL: $(S2S_SRC_FILE_ACCOUNT_SAS_URL)
          #S2S_SRC_S3_SERVICE_URL: $(S2S_SRC_S3_SERVICE_URL)
          #S2S_SRC_GCP_SERVICE_URL: $(S2S_SRC_GCP_SERVICE_URL)
          #SHARE_SAS_URL: $(SHARE_SAS_URL)
      #- script: |
       #   pip install azure-storage-blob==12.0.0b3
          # the recent release 1.0.0b4 has a breaking change
        #  pip install azure-core==1.0.0b3
         # python ./tool_distributed_mutex.py unlock "$(MUTEX_URL)"
        #name: 'Release_the_distributed_mutex'
        # this runs even if the job was canceled (only if the mutex was acquired by this job)
        #condition: and(always(), eq(variables['isMutexSet'], 'true'))
